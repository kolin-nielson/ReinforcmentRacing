behaviors:
  RacingBehavior:
    trainer_type: ppo
    hyperparameters:
      batch_size: 2048
      buffer_size: 40960 # Good size for diverse experiences
      learning_rate: 0.0002 # Stable learning rate
      beta: 0.008       # Slightly reduced exploration - prioritize learning safe baseline first
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      memory:
        sequence_length: 128  # Essential for driving context
        memory_size: 256
    reward_signals:
      extrinsic:
        gamma: 0.995        # Standard high discount factor
        strength: 1.0
      # --- Curiosity Disabled ---
      # curiosity:
      #   strength: 0.0
      # --- End Curiosity Disabled ---
    max_steps: 30000000     # Adjust higher (e.g., 50M+) if training on many tracks or needing more time
    time_horizon: 192
    summary_freq: 25000
    checkpoint_interval: 500000
    threaded: true

# Notes:
# - Remember to use the refined V11 Inspector settings in Unity.
# - CRITICAL: Train on multiple diverse tracks for adaptability.
# - Monitor training: If agent is TOO safe/slow and learning plateaus, consider slightly increasing beta (e.g., back to 0.01 or 0.012).